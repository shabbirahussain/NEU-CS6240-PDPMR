---
title: "Assignment Report - A8(A7)"
author: "Shabbir"
date: "October 31, 2017"
output:
  html_document: default
  pdf_document:
    fig_crop: no
    fig_width: 5
    latex_engine: xelatex
---

```{r setup, echo=F, results='hide',message=F, warning=F, cache=F}
library("ggplot2")
library("knitr")
library("kableExtra")
library("scales")
library("plyr")
library("microbenchmark")
library("reshape2")
library("readr")

library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("grid")
library("gridBase")
library("gridExtra")
library("dplyr")
library("png")
```

```{r cache=TRUE,echo=F, results = 'asis', warning=F, message=F}
data <- read.csv('results/st1/kmeans/clust-merged.csv', sep=";", header = TRUE)
```
```{r cache=TRUE,echo=F, results = 'asis', warning=F, message=F}

niceLabels <- function(x) ifelse(x>=1000000, paste0(x/1000000,"mln"), ifelse(x==0, x, paste0(x/1000,"k")))

plotClust1D <- function(data, subset_name, labArr, rounding=0, scaling=1, title="", xlabel="", ylabel="", ysuffix="", xsuffix=""){
  dataP <- subset(data, PROB_NAME==subset_name)
  
  minCId = dataP[(order( dataP$X)), ][1, ]$CLUSTER_ID
  maxCId = dataP[(order(-dataP$X)), ][1, ]$CLUSTER_ID
  
  dataP <- dataP %>% mutate(CLUSTER_ID = ifelse(CLUSTER_ID == minCId, labArr[1], ifelse(CLUSTER_ID == maxCId, labArr[3], labArr[2])))
  
  cntByCltrX   <- dataP %>% group_by(CLUSTER_ID, X=round(X/scaling, rounding)) %>% count
  xMeanByCltr  <- cntByCltrX %>% group_by(CLUSTER_ID) %>% summarise(X_MEAN = round(mean(X), 0)) 
  cntByCltrX   <- merge(cntByCltrX, xMeanByCltr, by = "CLUSTER_ID")
  
  cntByCltr    <- cntByCltrX %>% group_by(CLUSTER_ID) %>% summarise(n = sum(n)) 
  cntByCltr$n  <- round(cntByCltr$n * 100/ sum(cntByCltr$n), 0)
  arrange(cntByCltr, desc(CLUSTER_ID))
  
  cntByCltrY <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("CLUSTER_ID", "Y", "LABEL"))
  tot = 0
  for(i in 1:nrow(cntByCltr)) {
      clstrRow <- cntByCltr[i,]
      for(j in 1:clstrRow$n){
        tot = tot+1
        label <- ""
        if (j==round(clstrRow$n/2,0)) {label <- substring(clstrRow$CLUSTER_ID, 2)}
        cntByCltrY[nrow(cntByCltrY) + 1,] = list(clstrRow$CLUSTER_ID, tot, label)
      }
  }
  cntByCltrXY <- merge(cntByCltrX, cntByCltrY, by = "CLUSTER_ID")
  cntByCltrXY <- cntByCltrXY %>% mutate(LABEL = ifelse(X == X_MEAN, LABEL, ""))
  
  
  ggplot(data=cntByCltrXY, aes(x=X, y=Y, fill=n)) +
    geom_tile(width=1, height=1) +
    scale_fill_gradient(low="grey", high="black", name="Songs") +
    labs(x="", y="Cummulative % of Songs", title=title) +
    scale_color_continuous(labels=niceLabels) +
    scale_x_continuous(labels=function(x) dollar_format(suffix = xsuffix, prefix = "")(x)) +
    scale_y_continuous(labels=function(x) dollar_format(suffix = " %", prefix = "")(x)) +
    geom_text(aes(x=max(X)+5, y=Y, label=paste(LABEL, "", " ")), size=4) + 
    theme_bw()
}


plotClust2D <- function(data, subset_name, labArr, rounding=0, scaling=1, title="", xlabel="", ylabel="", ysuffix="", xsuffix=""){
  dataP <- subset(data, PROB_NAME==subset_name)
  dataP <- dataP %>% mutate(X = round(X/scaling, rounding), Y = round(Y/scaling, rounding))
  
  
  minCId = dataP[(order( dataP$X)), ][1, ]$CLUSTER_ID
  maxCId = dataP[(order(-dataP$X)), ][1, ]$CLUSTER_ID
  
  dataP <- dataP %>% mutate(CLUSTER_ID = ifelse(CLUSTER_ID == minCId, labArr[1], ifelse(CLUSTER_ID == maxCId, labArr[3], labArr[2])))
  
  plt.comb <- ggplot(data=dataP, aes(x=X, y=Y, color=CLUSTER_ID)) +
    geom_density_2d() +
    labs(x=xlabel, y=ylabel, title=title) +
    scale_x_continuous(labels=function(x) dollar_format(suffix = xsuffix, prefix = "")(x)) +
    scale_y_continuous(labels=function(x) dollar_format(suffix = " %", prefix = "")(x)) +
    scale_color_manual(values=brewer.pal(4, "Greys")[2:4]) + 
    theme(legend.position="none", panel.background = element_blank(), axis.line = element_line(colour = "black"), panel.grid.major = element_line(color="lightgrey"))
  
  cntByCltr <- dataP %>% group_by(CLUSTER_ID) %>% count
  plt.cnt <- 
    ggplot(data=cntByCltr, aes(x="", y=n, fill=CLUSTER_ID)) +
    geom_bar(width=1, stat="identity", position=position_stack(reverse = T)) +
    geom_label(aes(label=paste0(CLUSTER_ID, ": ", format(n/sum(n)*100, digits=0), "%")), 
               position=position_stack(reverse = T)) +
    labs(title="", subtitle="", y="Songs") +
    scale_y_continuous(position="right", labels=niceLabels) +
    theme(axis.title.x=element_blank(), legend.position="none", axis.ticks.x=element_blank(), panel.background = element_blank()) +
     scale_fill_manual(values=brewer.pal(4, "Greys")[2:4]) 
 
  grid.arrange(plt.comb, plt.cnt, ncol=2, widths=c(5,1))
}

```

### Abstract

This report describes results of different kinds of clustering on the million songs dataset. Detailed requirements can be found [here](http://janvitek.org/pdpmr/f17/task-a7-clustering.html). The first section describes the experiment design, setup and brief structure of project. Followed by second section starts with results visualization and analysis of subtasks including run time statistics. Finally we conclude with findings from both sections.

# 1. Design of experiment

<table><tr><td>
```{r cache=TRUE, fig.cap="Figure 1", fig.align='center', fig.width=20, fig.height=10, echo=F}
img <- readPNG("docs/images/DesignDiagram.png")
grid.raster(img)
```
</td><td valign="top">
Our clustering architecture for subtask 1 is a single 2d clustering algorithm which takes 2d points RDD and returns 3 clusters from those points as set of points assigned to centeroids. For each problem within subtask we filtered data and passed an RDD of just $(x,y)$ coordinates. For 1d clustering $y$ coordinate is set to static value.

Case classes were intentionally not used in the code as it made the code lines longer and incur added penalty for pattern matching. Instead tuples with comments convention is used throughout the code.

We ran all experiments on a MacBookPro12 with 1 Intel Core i5 processor (2.7 GHz, 2 cores) with 8 GB ram.
</td></tr></table>

# 2. Results

## 2.1 Subtask 1

We ran K means clustering on the million song dataset until convergence. Convergence criteria used is when centroids don't change over 2 consecutive iterations. We found clustering takes 5 minutes till convergence for all tasks in subtask 1.

### 2.1.1 1D clustering

Figure 2 shows various clusters found using K-Means on million songs. The graphs shown are step heat maps. Here intensity of color shows the amount of songs concentrated at particular interval(bucket). As data is fractional we clubbed neighboring points together using buckets. Title of the plot shows bucket size used within that graph. Along the Y-axis we have percentage of songs. Height of the heat-step specifies the amount of songs in that cluster. Added together they span full corpus (100% songs). Relative size of clusters gives an indication of total number of songs present in that cluster. Every step specifies start of a different cluster, whose name can be found on the Y2 axis (right-hand-side).


```{r cache=TRUE,fig.align='center', fig.cap="Figure 2", echo=FALSE, results = 'asis', warning=FALSE, message=FALSE, fig.height=3.5, fig.width=10}
plt.loudness <- plotClust1D(data, subset_name="L", labArr=c("1 Quiet", "2 Med", "3 Loud"), title="Fuzzy Loudness (1 db buckets)", xlabel="Loudness", xsuffix=" db", rounding=0)

plt.length <- plotClust1D(data, subset_name="N", labArr=c("1 Short", "2 Med", "3 Long"), title="Fuzzy Length (30 sec buckets)", xlabel="Length", xsuffix=" min", scaling=30)

plt.tempo <- plotClust1D(data, subset_name="T", labArr=c("1 Slow", "2 Med", "3 Fast"), title="Fuzzy Tempo (3 BPM buckets)", xlabel="Tempo", xsuffix=" BPM", rounding=0, scaling=3)

plt.hot <- plotClust1D(data, subset_name="H", labArr=c("1 Cool", "2 Mild", "3 Hot"), title="Fuzzy Song Hotness (1% buckets)", xlabel="Hotness", xsuffix=" %", rounding=3, scaling=.01)

grid.arrange(plt.loudness, plt.length, plt.tempo, plt.hot, ncol=2, nrow=2)

```

#### Observations
* Clusters have some overlapping in the transition bucket as we are intentionally removing precision while reporting.
* We see there are a lot more loud songs (more than 51%) and just a few quite songs. 
* Most of the density of songs is concentrated within -9 to -5 db in loud songs.
* There is more concentration of songs between 25BPM to 50BPM tempo. Hinting towards a sweet range of listening for audience. 
* Most songs lie in the 7-9 Minutes range.
* Interestingly there exists some density of songs all over from 20 plus minutes to 60 minutes.
* Most songs are pretty evenly distributed in the hotness spectrum with gradual gradient transition.
* There seems to be a single odd dense region in the cool songs around 26% which doesn't seem to have a gradual transition. 

### 2.1.2 2D Clustering

Here we cluster songs based on artist hotness and song hotness. The graph in Figure 3 shows a density plot. The plot is segregated by shades of grey signifying different cluster categories. Graph also has a legend on the side showing the color to category correlation along with relative percentage of songs within them.

<table><tr><td valign="top">

#### Observations
* There seems to a correlation between artist hotness and song hotness. This can be seen from the lack of hot songs near less than 20% artist hotness.
* When artist hotness is between 20-60% songs hotness can range from 20-100%.
* There is a large concentrations of songs near 40% artist hotness and 50% song hotness. 

</td><td>
```{r cache=TRUE, echo=FALSE, results = 'asis', warning=F, message=F,fig.cap="Figure 3", fig.height=5, fig.width=10}

plotClust2D(data, subset_name="C", labArr=c("1 Cool", "2 Mild", "3 Hot"), title="Combined Hotness (1 % buckets)", xsuffix=" %", rounding=0, scaling = 0.01, xlabel="Artist Hotness", ylabel="Song Hotness")
```
</td></tr></table>

### 2.1.3 Agglomerative 1D Clustering

We implemented agglomerative clustering technique with spark on 1D data points using following version of algorithm:

1. Sort all points and assign an index to every point using `zipWithIndex` and join it with consecutive indices to compute their distances.
1. Loop until count(unClusteredPoints) is greater than required number of clusters
    1. Find minimum dist in unClusteredPoints.
    1. Filter out **all** elements that have the same distance as min dist from unClusteredPoints. (Cluster multiple points in single iteration).
    1. The above assumption is valid as all points are sorted and are 1D. And after merging new cluster distance to any neighbor could never be less than min distance found before. (This helps us speed up algo by many folds.)
1. Finally we are left with a minimal set of points in the unClusteredPoints RDD.
1. We set original point distances to $+\inf$ and compute final cluster assignment by replacing these $+\inf$ distances with unClusteredPoints.
1. Then do a groupBy and collect all points on the driver to sequentially loop through them.
1. In this design whenever we find a non infinite value (point from the unclustered set) we get a cluster boundary.

Below figure shows a comparison of Agglomerative clustering output vs Kmeans for Fuzzy loudness on the million songs dataset.
```{r cache=TRUE,echo=F, results = 'asis', warning=F, message=F}
data1 <- read.csv('results/st1/agglo/clust-merged.csv', sep=";", header = TRUE)
```
```{r cache=TRUE,fig.align='center', fig.cap="Figure 4", echo=FALSE, results = 'asis', warning=FALSE, message=FALSE, fig.height=2, fig.width=10}
plt.Aloudness <- plotClust1D(data1, subset_name="AL", labArr=c("1 Quiet", "2 Med", "3 Loud"), title="Agglomerative Loudness (1 db buckets)", xlabel="Loudness", xsuffix=" db", rounding=0)

plt.Kloudness <- plotClust1D(data1, subset_name="KL", labArr=c("1 Quiet", "2 Med", "3 Loud"), title="KMeans Loudness (1 db buckets)", xlabel="Loudness", xsuffix=" db", rounding=0)

grid.arrange(plt.Aloudness, plt.Kloudness, ncol=2, nrow=1)

```

#### Observations

* We found that Agglomerative clustering is difficult to parallelize due to one update at a time constraint. We can parallelize the search for updates and filters. However we still have to go through a lot of iterations to achieve the final clustering result.
* It took twice as much time as Kmeans to run on the million songs dataset. (K-Means took 70sec, while, agglomerative took 137sec.)
* We can also see some imbalance between the clusters found by Agglomerative and K-Means. It comes from the fact that Agglomerative chooses to merge the closest clusters first, while K-Means picks centroids and tries to assign points to it while updating the centroids.

## 2.2 Subtask 2

The goal of this assignment is to cluster artists based on their genre.

#### Assumptions and Specifications

* **Our centroids are not contrained to be a valid point in the graph.**: It could be a subset of any combination of *terms*. Thus, we cannot use the artist similarity to construct edges between artist and centroids. In our case we assume all artists are connected to each other iff they have at least $1$ common term. And distance between them is the inverse of the intersection of matching *terms*.
* We assume artist `A` is similar to artist `B` reverse is **NOT** true. As in case of $A\subset B$.
* `KMode` : It is based on KMode clustering technique.
      1. Initially we assign $1$ as weight to all terms associated with any artist.
      1. We then take intersect count of *artist terms* to *centroid terms* as a distance measure.
      1. To calculate new centroid position we take mode (most repeating values) of, ***set of artist terms***, for all artists belonging to that cluster.
      1. Finally, we calculate distance to this new centroid by using same intersection technique.

#### Visualization

Key information to read below figure:

1. All clusters are shown as bunch of tag clouds.
1. Every tag cloud is in row major format. And are sorted by `CLUSTER_ID`.
1. The size of the word indicates the number of artists assigned to that cluster.
1. While the color of the word(darker grey) cluster represents that term is agreeing with the respective cluster centroid.
1. Note: Some centroids are not plotted due to shortage of screen space as decided by wordcloud library!

```{r cache=TRUE, echo=FALSE, results = 'asis', warning=FALSE, message=FALSE}
clustWord <- read.csv("results/st2/clstr_term_freq-merged.csv", sep=";", header = TRUE)
clustCent <- read.csv("results/st2/centroids-merged.csv", sep=";", header = TRUE)

clustCent$COUNT <- 1
clustCent$IS_CENT <- factor(1, c(0, 1, 2))
clustCent$COUNT <- ceiling(1/(nchar(as.character(clustCent$TERMS))^2) * 1000) 

clustWord$IS_CENT <- as.factor(clustWord$IS_CENT)
#clustWord <- subset(clustWord, IS_CENT=="true")

iters = unique(clustWord$ITER)
lenIt = length(iters)
clstr = sort(unique(clustWord$CLUSTER_ID))
lenCl = ceiling(sqrt(length(clstr)))
```

```{r cache=TRUE, echo=FALSE, results = 'asis', warning=FALSE, message=FALSE}
drawCloud <- function(clustWord){
  wordcloud(words = clustWord$TERM, freq = clustWord$COUNT, min.freq = 1,
    max.words=200, random.order=F, rot.per=0.35, ordered.colors=TRUE,
    colors=brewer.pal(3, "Greys")[clustWord$IS_CENT])
  
}

plotAllClusters <- function(clustWord, i){
  clustWordI <- subset(clustWord, ITER==i)
  par(mfrow=c(lenCl, lenCl), mar=rep(0, 4))
  for (j in clstr){
    clustWordIJ <- subset(clustWordI, CLUSTER_ID==j)
    if (nrow(clustWordIJ) == 0) {
      clustWordIJ[1,] = list(i, j, "",  1, TRUE)
    } 
    drawCloud(clustWordIJ)
  }
}

```

##### Cluster Assignment 
<table><tr><th valign="top" align="center">First Iteration</th><th>Last Iteration</th></tr>
</tr><td valign="top" align="center" style="border: 1px #000000; border-style: none solid none none;">
```{r cache=TRUE, error=FALSE, warning=FALSE, echo=FALSE, fig.height = 16, fig.cap="Figure 5.1", fig.width = 16, fig.align = "center"}
plotAllClusters(clustWord, min(clustCent$ITER))
```
</td><td valign="top" align="center">
```{r cache=TRUE, error=FALSE, warning=FALSE, echo=FALSE, fig.height = 16, fig.cap="Figure 5.2", fig.width = 16, fig.align = "center"}
plotAllClusters(clustWord, max(clustCent$ITER))
```
</td></tr></table>

#### Observations

* As algorithm progresses the clusters seems to merge together to form a bigger distinction between *terms*.
* This can be seen in the case of `POP` which was prominent in every cluster earlier in first iteration. It is later aggregated to its own *term* towards the end of the algorithm.
* Clustering also found a related genre of comedy with *terms* like *funny*, *commedian*, *comedy* etc. 
* There is a strong correlation between *rap* and *hip-hop* as seen in the last iteration where both terms are present together with almost equal weight.

## Conclusion

1. From this exercise it can be seen K-Means is much more faster than the Agglomerative approach used here. 
1. It can become difficult to compute Agglomerative for multidimensional points as every iteration Cartesian product has to calculated to compute all pairs distance.
1. Using simultaneous update of multiple min length points million songs clustering size was shrunk from $999,056$ songs to just $32,828$ songs in the first iteration itself. This optimization can be very useful for 1D data points.
1. K-Mode clustering shows promising results in terms of end clusters with much more specialized keywords for centroids after few iterations compared to the start of the algorithm. It would be interesting to explore and contrast results of some other algorithm on the same dataset.
